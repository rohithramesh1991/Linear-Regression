# -*- coding: utf-8 -*-
"""Linear regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ceF0ueoKlcSMNIvhMRfSnjNR3_yyVwp5
"""

import pandas as pd
import numpy as np

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split

from sklearn.datasets import load_boston
boston = load_boston()
print(boston.data.shape)

boston.keys()

"""**Data**"""

pd.DataFrame(boston.data).head()

"""**Target**"""

pd.DataFrame(boston.target).head()

"""**Feature Names**"""

print(boston.feature_names)

"""**Description**"""

print(boston.DESCR)

boston_data = pd.DataFrame(boston.data)
print(boston_data.head())

boston_data.columns = boston.feature_names
print(boston_data.head())

boston_target = pd.DataFrame(boston.target)
print(boston_target.head())

boston_data['MEDV']=boston.target
print(boston_data.head())

import matplotlib.pyplot as plt
plt.hist(boston.target)
plt.title('Boston Housing Prices and Count Histogram')
plt.xlabel('price ($1000s)')
plt.ylabel('count')
plt.show()

"""**The below will create a scatterplot on each of the feature_names versus the price of Boston housing.**"""

import seaborn as sns
sns.pairplot(boston_data,x_vars=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'], y_vars=["MEDV"])

boston_data.describe()

boston_data.isnull().sum(axis=0)

X = boston_data.drop('MEDV', axis = 1)
Y = boston_data['MEDV']

X.shape

Y.shape

X.head()

Y.head()

"""**Least square linear regression in scikit-learn**"""

x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 0)
Linreg = LinearRegression().fit(x_train, y_train)
print("Linear model intercept(b):{}".format(Linreg.intercept_))
print("Linear model coeff (w):{}".format(Linreg.coef_))
print("R-squared score(training): {:.3f}".format(Linreg.score(x_train,y_train)))
print("R-squared score(test): {:.3f}".format(Linreg.score(x_train,y_train)))

y_pred = Linreg.predict(x_test)

plt.scatter(y_test, y_pred)
plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $\hat{Y}_i$")
plt.title("Prices vs Predicted prices: $Y_i$ vs $\hat{Y}_i$")

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print(mse)

#That means that the model isn’t a really great linear model.

"""**Ridge Linear Regression**"""

#Using a scalar object : fit and transform method

Scaler = MinMaxScaler()
X_train_Scaled = Scaler.fit_transform(x_train)
X_test_Scaled = Scaler.transform(x_test)
clf = Ridge().fit(X_train_Scaled , y_train)
R2_score = clf.score(X_test_Scaled, y_test)

#Ridge regression with regularization parameter: alpha

print('Ridge regression : effect of alpha regularization parameter \n')
for this_alpha in [0.001,0.1,0,1,10,20,50,100,1000]:
    linridge = Ridge(alpha = this_alpha).fit(X_train_Scaled,y_train)
    r2_train = linridge.score(X_train_Scaled, y_train)
    r2_test = linridge.score(X_test_Scaled, y_test)
    num_coeff_bigger = np.sum(abs(linridge.coef_)>1.0)
    print('Alpha = {:.2f}\n numabs(coeff) >1.0:{}, r->squared training: {:.2f}, r_squared test: {:.2f}\n'.format(this_alpha,num_coeff_bigger,r2_train,r2_test))

"""**Lasso Regression**"""

linlasso = Lasso(alpha=0.0001, max_iter=10e5).fit(X_train_Scaled,y_train)
print('lasso regression linear model intercept:{}'.format(linlasso.intercept_))
print('lasso regression linear model coeff:\n{}'.format(linlasso.coef_))
print('Non-zero feature :{}'.format(np.sum(linlasso.coef_!=0)))
print('R-Squared score(training):{:.3f}'.format(linlasso.score(X_train_Scaled, y_train)))
print('R-Squared score(test):{:.3f}\n'.format(linlasso.score(X_test_Scaled,y_test)))

"""**Polynomial Feature with Linear Regression**"""

Poly = PolynomialFeatures(degree =2)
X_F1_poly = Poly.fit_transform(X)
x_poly_train, x_poly_test, y_train, y_test = train_test_split(X_F1_poly, Y,test_size=0.33, random_state = 0)

"""**Linear Regression with polynomial features upto degree 2**"""

linreg = LinearRegression().fit(x_poly_train, y_train)
print('(poly deg 2) Linear model coeff(W):\n{}'.format(linreg.coef_))
print('(poly deg 2) Linear model intercept(b):{:.3f}'.format(linreg.intercept_))
print('(poly deg 2) R-Squared score (training):{:.3f}'.format(linreg.score(x_poly_train, y_train)))
print('(poly deg 2) R-Squared score (test):{:.3f}'.format(linreg.score(x_poly_test, y_test)))
print('\n Addition of many polynomial features often leads to overfitting, so we often use polynomial feature in combination with regression that has a regularization penalty, like ridge regression')

"""**Ridge Regression with polynomial features upto degree 2**"""

Poly = PolynomialFeatures(degree =2)
X_F1_poly1 = Poly.fit_transform(X)
x_poly_train1, x_poly_test1, y_train, y_test = train_test_split(X_F1_poly1, Y, test_size=0.4, random_state = 0)

linrid = Ridge(alpha = 0.001).fit(x_poly_train1, y_train)
print('(poly deg 2) Linear model coeff(W):\n{}'.format(linrid.coef_))
print('(poly deg 2) Linear model intercept(b):{:.3f}'.format(linrid.intercept_))
print('(poly deg 2) R-Squared score (training):{:.3f}'.format(linrid.score(x_poly_train1, y_train)))
print('(poly deg 2) R-Squared score (test):{:.3f}'.format(linrid.score(x_poly_test1, y_test)))

print('(poly deg 2) Linear model intercept(b):5.418')
print('(poly deg 2) R-Squared score (training):0.88')
print('(poly deg 2) R-Squared score (test):0.85')